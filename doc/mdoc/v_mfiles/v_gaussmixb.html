<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
  <title>Description of v_gaussmixb</title>
  <meta name="keywords" content="v_gaussmixb">
  <meta name="description" content="V_GAUSSMIXB approximate Bhattacharyya divergence between two GMMs">
  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
  <meta name="generator" content="m2html &copy; 2003 Guillaume Flandin">
  <meta name="robots" content="index, follow">
  <link type="text/css" rel="stylesheet" href="../m2html.css">
</head>
<body>
<a name="_top"></a>

<!-- index.html v_mfiles -->
<h1>v_gaussmixb

</h1>

<h2><a name="_name"></a>PURPOSE <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
<div class="box"><strong>V_GAUSSMIXB approximate Bhattacharyya divergence between two GMMs</strong></div>

<h2><a name="_synopsis"></a>SYNOPSIS <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
<div class="box"><strong>function [d,dbfg]=v_gaussmixb(mf,vf,wf,mg,vg,wg,nx) </strong></div>

<h2><a name="_description"></a>DESCRIPTION <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
<div class="fragment"><pre class="comment">V_GAUSSMIXB approximate Bhattacharyya divergence between two GMMs

 Usage: (1) d=v_gaussmixb(mf,vf,wf,mg,vg,wg);        % Estimate Bhattacharyya divergence between {mf,vf,wf} and {mg,vg,wg}
                                                     % vf and vg can independently be full or diagonal covariances

        (2) [d,dbfg]=v_gaussmixb(mf,vf,wf,mg,vg,wg); % Also calculate exact Bhattacharyya divergence between compnents of f and components of g

        (3) d=v_gaussmixb(mf,vf,wf,mg,vg,wg,0);      % Calculate upper bound to Bhattacharyya divergence

        (4) [d,dbfg]=v_gaussmixb(mf,vf,wf);          % Calculate Bhattacharyya divergence between compnents of f. d=0 always in this case.

        (5) v_gaussmixb(mf,vf,wf,mg,vg,wg);          % Plot gra[hs of distributions (dimension p must equal 1)

 Inputs: with kf &amp; kg mixtures, p data dimensions

   mf(kf,p)                mixture means for GMM f
   vf(kf,p) or vf(p,p,kf)  variances (diagonal or full) for GMM f [default: identity]
   wf(kf,1)                weights for GMM f - must sum to 1 [default: uniform]
   mg(kg,p)                mixture means for GMM g [g=f if mg,vg,wg omitted]
   vg(kg,p) or vg(p,p,kg)  variances (diagonal or full) for GMM g [default: identity]
   wg(kg,1)                weights for GMM g - must sum to 1 [default: uniform]
   nx                      number of samples to use in importance sampling [default: 1000]
                           Set nx=0 to save computation by returning only an upper bound to the Bhattacharyya divergence.

 Outputs:
   d             the approximate Bhattacharyya divergence D_B(f,g)=-log(Int(sqrt(f(x)g(x)) dx)).
                 if nx=0 this will be an upper bound (typically 0.3 to 0.7 too high) rather than an estimate.
   dbfg(kf,kg)   the exact Bhattacharyya divergence between the unweighted components of f and g

 The Bhattacharyya divergence, D_B(f,g), between two distributions, f(x) and g(x), is -log(Int(sqrt(f(x)g(x)) dx)).
 It is a special case of the Chernoff Bound [2]. The Bhattacharyya divergence [1] satisfies:
     (1) D_B(f,g) &gt;= 0
     (2) D_B(f,g) = 0 iff f = g
     (3) D_B(f,g) = D_B(g,f)
 It is not a distance because it  does not satisfy the triangle inequality. It upper bounds the Bayes
 divergence -log(Int(min(f(x),g(x)) dx) which relates to the probability of 2-class misclassification [1].

 This routine calculates the &quot;variational importance sampling&quot; estimate of (or if nx=0,
 the &quot;variational II&quot; upper bound to) the Bhattacharyya divergence from [3]. It is exact
 when f and g are single component gaussians and is zero if f=g.

 Refs:
 [1] T. Kailath. The divergence and Bhattacharyya distance measures in signal selection.
     IEEE Trans Communication Technology, 15 (1): 52–60, Feb. 1967.
 [2] H. Chernoff. A measure of asymptotic efficiency for tests of a hypothesis based on the
     sum of observations. The Annals of Mathematical Statistics, 23 (4): 493–507, Dec. 1952.
 [3] P. A. Olsen and J. R. Hershey. Bhattacharyya error and divergence using variational
     importance sampling. In Proc. Interspeech Conf., pages 46–49, 2007.

       Copyright (C) Mike Brookes 2024
      Version: $Id: v_gaussmixk.m 10865 2018-09-21 17:22:45Z dmb $

   VOICEBOX is a MATLAB toolbox for speech processing.
   Home page: http://www.ee.ic.ac.uk/hp/staff/dmb/voicebox/voicebox.html

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
   This program is free software; you can redistribute it and/or modify
   it under the terms of the GNU General Public License as published by
   the Free Software Foundation; either version 2 of the License, or
   (at your option) any later version.

   This program is distributed in the hope that it will be useful,
   but WITHOUT ANY WARRANTY; without even the implied warranty of
   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
   GNU General Public License for more details.

   You can obtain a copy of the GNU General Public License from
   http://www.gnu.org/copyleft/gpl.html or by writing to
   Free Software Foundation, Inc.,675 Mass Ave, Cambridge, MA 02139, USA.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%</pre></div>

<!-- crossreference -->
<h2><a name="_cross"></a>CROSS-REFERENCE INFORMATION <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
This function calls:
<ul style="list-style-image:url(../matlabicon.gif)">

<li><a href="v_gaussmixp.html" class="code" title="function [lp,rp,kh,kp]=v_gaussmixp(y,m,v,w,a,b)">v_gaussmixp</a>	V_GAUSSMIXP calculate probability densities from or plot a Gaussian mixture model</li>
<li><a href="v_logsum.html" class="code" title="function y=v_logsum(x,d,k)">v_logsum</a>	V_LOGSUM v_logsum(x,d,k)=log(sum(k.*exp(x),d))</li>
<li><a href="v_randvec.html" class="code" title="function [x,kx]=v_randvec(n,m,c,w,mode)">v_randvec</a>	V_RANDVEC  Generate real or complex GMM/lognormal random vectors X=(N,M,C,W,MODE)</li>
<li><a href="v_texthvc.html" class="code" title="function h=v_texthvc(x,y,t,p,q,r)">v_texthvc</a>	V_TEXTHVC - write text on graph with specified alignment and colour</li>
</ul>
This function is called by:
<ul style="list-style-image:url(../matlabicon.gif)">

</ul>
<!-- crossreference -->




<h2><a name="_source"></a>SOURCE CODE <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
<div class="fragment"><pre>0001 <a name="_sub0" href="#_subfunctions" class="code">function [d,dbfg]=v_gaussmixb(mf,vf,wf,mg,vg,wg,nx)</a>
0002 <span class="comment">%V_GAUSSMIXB approximate Bhattacharyya divergence between two GMMs</span>
0003 <span class="comment">%</span>
0004 <span class="comment">% Usage: (1) d=v_gaussmixb(mf,vf,wf,mg,vg,wg);        % Estimate Bhattacharyya divergence between {mf,vf,wf} and {mg,vg,wg}</span>
0005 <span class="comment">%                                                     % vf and vg can independently be full or diagonal covariances</span>
0006 <span class="comment">%</span>
0007 <span class="comment">%        (2) [d,dbfg]=v_gaussmixb(mf,vf,wf,mg,vg,wg); % Also calculate exact Bhattacharyya divergence between compnents of f and components of g</span>
0008 <span class="comment">%</span>
0009 <span class="comment">%        (3) d=v_gaussmixb(mf,vf,wf,mg,vg,wg,0);      % Calculate upper bound to Bhattacharyya divergence</span>
0010 <span class="comment">%</span>
0011 <span class="comment">%        (4) [d,dbfg]=v_gaussmixb(mf,vf,wf);          % Calculate Bhattacharyya divergence between compnents of f. d=0 always in this case.</span>
0012 <span class="comment">%</span>
0013 <span class="comment">%        (5) v_gaussmixb(mf,vf,wf,mg,vg,wg);          % Plot gra[hs of distributions (dimension p must equal 1)</span>
0014 <span class="comment">%</span>
0015 <span class="comment">% Inputs: with kf &amp; kg mixtures, p data dimensions</span>
0016 <span class="comment">%</span>
0017 <span class="comment">%   mf(kf,p)                mixture means for GMM f</span>
0018 <span class="comment">%   vf(kf,p) or vf(p,p,kf)  variances (diagonal or full) for GMM f [default: identity]</span>
0019 <span class="comment">%   wf(kf,1)                weights for GMM f - must sum to 1 [default: uniform]</span>
0020 <span class="comment">%   mg(kg,p)                mixture means for GMM g [g=f if mg,vg,wg omitted]</span>
0021 <span class="comment">%   vg(kg,p) or vg(p,p,kg)  variances (diagonal or full) for GMM g [default: identity]</span>
0022 <span class="comment">%   wg(kg,1)                weights for GMM g - must sum to 1 [default: uniform]</span>
0023 <span class="comment">%   nx                      number of samples to use in importance sampling [default: 1000]</span>
0024 <span class="comment">%                           Set nx=0 to save computation by returning only an upper bound to the Bhattacharyya divergence.</span>
0025 <span class="comment">%</span>
0026 <span class="comment">% Outputs:</span>
0027 <span class="comment">%   d             the approximate Bhattacharyya divergence D_B(f,g)=-log(Int(sqrt(f(x)g(x)) dx)).</span>
0028 <span class="comment">%                 if nx=0 this will be an upper bound (typically 0.3 to 0.7 too high) rather than an estimate.</span>
0029 <span class="comment">%   dbfg(kf,kg)   the exact Bhattacharyya divergence between the unweighted components of f and g</span>
0030 <span class="comment">%</span>
0031 <span class="comment">% The Bhattacharyya divergence, D_B(f,g), between two distributions, f(x) and g(x), is -log(Int(sqrt(f(x)g(x)) dx)).</span>
0032 <span class="comment">% It is a special case of the Chernoff Bound [2]. The Bhattacharyya divergence [1] satisfies:</span>
0033 <span class="comment">%     (1) D_B(f,g) &gt;= 0</span>
0034 <span class="comment">%     (2) D_B(f,g) = 0 iff f = g</span>
0035 <span class="comment">%     (3) D_B(f,g) = D_B(g,f)</span>
0036 <span class="comment">% It is not a distance because it  does not satisfy the triangle inequality. It upper bounds the Bayes</span>
0037 <span class="comment">% divergence -log(Int(min(f(x),g(x)) dx) which relates to the probability of 2-class misclassification [1].</span>
0038 <span class="comment">%</span>
0039 <span class="comment">% This routine calculates the &quot;variational importance sampling&quot; estimate of (or if nx=0,</span>
0040 <span class="comment">% the &quot;variational II&quot; upper bound to) the Bhattacharyya divergence from [3]. It is exact</span>
0041 <span class="comment">% when f and g are single component gaussians and is zero if f=g.</span>
0042 <span class="comment">%</span>
0043 <span class="comment">% Refs:</span>
0044 <span class="comment">% [1] T. Kailath. The divergence and Bhattacharyya distance measures in signal selection.</span>
0045 <span class="comment">%     IEEE Trans Communication Technology, 15 (1): 52–60, Feb. 1967.</span>
0046 <span class="comment">% [2] H. Chernoff. A measure of asymptotic efficiency for tests of a hypothesis based on the</span>
0047 <span class="comment">%     sum of observations. The Annals of Mathematical Statistics, 23 (4): 493–507, Dec. 1952.</span>
0048 <span class="comment">% [3] P. A. Olsen and J. R. Hershey. Bhattacharyya error and divergence using variational</span>
0049 <span class="comment">%     importance sampling. In Proc. Interspeech Conf., pages 46–49, 2007.</span>
0050 <span class="comment">%</span>
0051 <span class="comment">%       Copyright (C) Mike Brookes 2024</span>
0052 <span class="comment">%      Version: $Id: v_gaussmixk.m 10865 2018-09-21 17:22:45Z dmb $</span>
0053 <span class="comment">%</span>
0054 <span class="comment">%   VOICEBOX is a MATLAB toolbox for speech processing.</span>
0055 <span class="comment">%   Home page: http://www.ee.ic.ac.uk/hp/staff/dmb/voicebox/voicebox.html</span>
0056 <span class="comment">%</span>
0057 <span class="comment">%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%</span>
0058 <span class="comment">%   This program is free software; you can redistribute it and/or modify</span>
0059 <span class="comment">%   it under the terms of the GNU General Public License as published by</span>
0060 <span class="comment">%   the Free Software Foundation; either version 2 of the License, or</span>
0061 <span class="comment">%   (at your option) any later version.</span>
0062 <span class="comment">%</span>
0063 <span class="comment">%   This program is distributed in the hope that it will be useful,</span>
0064 <span class="comment">%   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
0065 <span class="comment">%   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
0066 <span class="comment">%   GNU General Public License for more details.</span>
0067 <span class="comment">%</span>
0068 <span class="comment">%   You can obtain a copy of the GNU General Public License from</span>
0069 <span class="comment">%   http://www.gnu.org/copyleft/gpl.html or by writing to</span>
0070 <span class="comment">%   Free Software Foundation, Inc.,675 Mass Ave, Cambridge, MA 02139, USA.</span>
0071 <span class="comment">%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%</span>
0072 maxiter=15;                                                     <span class="comment">% maximum iterations for upper bound calculation</span>
0073 pruneth=0.2;                                                    <span class="comment">% prune threshold for importance sampling (prob that any excluded mixture would have been chosen)</span>
0074 [kf,p]=size(mf);
0075 <span class="keyword">if</span> nargin&lt;2 || isempty(vf)                                      <span class="comment">% if vf inoput is missing</span>
0076     vf=ones(kf,p);                                              <span class="comment">% ... default to diagonal covariances</span>
0077 <span class="keyword">end</span>
0078 <span class="keyword">if</span> nargin&lt;3 || isempty(wf)                                      <span class="comment">% if wf inoput is missing</span>
0079     wf=repmat(1/kf,kf,1);                                       <span class="comment">% ... default to uniform weights</span>
0080 <span class="keyword">end</span>
0081 <span class="keyword">if</span> p==1                                                         <span class="comment">% if data dimension is 1</span>
0082     vf=vf(:);                                                   <span class="comment">% ... then variances are always diagonal</span>
0083     dvf=true;
0084 <span class="keyword">else</span>
0085     dvf=ismatrix(vf) &amp;&amp; size(vf,1)==kf;                         <span class="comment">% diagonal covariance matrix vf is supplied</span>
0086 <span class="keyword">end</span>
0087 <span class="keyword">if</span> nargin&lt;7
0088     nx=1000;                                                    <span class="comment">% default count for importance sampling</span>
0089 <span class="keyword">end</span>
0090 hpl2=0.5*p*log(2);
0091 <span class="keyword">if</span> nargin&lt;5                                                     <span class="comment">% only f GMM specified: use this for g GMM as well</span>
0092     dbfg=zeros(kf,kf);                                          <span class="comment">% space for pairwise divergences</span>
0093     <span class="keyword">if</span> kf&gt;1
0094         nup=kf*(kf-1)/2;                                        <span class="comment">% number of elements in upper triangle</span>
0095         gix=ceil((1+sqrt(8*(1:nup)-3))/2);                      <span class="comment">% column of upper triangle</span>
0096         fix=(1:nup)-(gix-1).*(gix-2)/2;                         <span class="comment">% row of upper triangle</span>
0097         <span class="keyword">if</span> dvf                                                  <span class="comment">% diagonal covariances</span>
0098             mdif=mf(fix,:)-mf(gix,:);                           <span class="comment">% difference in means</span>
0099             vfpg=(vf(fix,:)+vf(gix,:));                         <span class="comment">% sum of variances</span>
0100             qldf=0.25*log(prod(vf,2));
0101             dbfg(fix+kf*(gix-1))=0.25*sum((mdif./vfpg).*mdif,2)+0.5*log(prod(vfpg,2))-qldf(fix)-qldf(gix)-hpl2; <span class="comment">% fill in upper triangle</span>
0102         <span class="keyword">else</span>                                                    <span class="comment">% full covariance matrices</span>
0103             qldf=zeros(kf,1);
0104             <span class="keyword">for</span> jf=1:kf                                         <span class="comment">% precalculate the log determinants for f covariances</span>
0105                 qldf(jf)=0.5*log(prod(diag(chol(vf(:,:,jf))))); <span class="comment">% equivalent to 0.25*log(det(vf(:,:,jg)))</span>
0106             <span class="keyword">end</span>
0107             <span class="keyword">for</span> jf=1:kf-1
0108                 vjf=vf(:,:,jf);                                 <span class="comment">% covariance matrix for f</span>
0109                 <span class="keyword">for</span> jg=jf+1:kf
0110                     vfg=vjf+vf(:,:,jg);
0111                     mdif=mf(jf,:)-mf(jg,:);                     <span class="comment">% difference in means</span>
0112                     dbfg(jf,jg)=0.25*(mdif/vfg)*mdif'+log(prod(diag(chol(vfg))))-qldf(jg)-qldf(jf)-hpl2; <span class="comment">% fill in upper triangle</span>
0113                 <span class="keyword">end</span>
0114             <span class="keyword">end</span>
0115         <span class="keyword">end</span>
0116         dbfg(gix+kf*(fix-1))=dbfg(fix+kf*(gix-1));              <span class="comment">% now reflect upper triangle divergences into the symmmetric lower triangle</span>
0117     <span class="keyword">end</span>
0118     d=0;                                                        <span class="comment">% divergence is always zero if f and g are identical</span>
0119 <span class="keyword">else</span>                                                            <span class="comment">% both f and g GMMs are specified as inputs</span>
0120     kg=size(mg,1);
0121     <span class="keyword">if</span> nargin&lt;5 || isempty(vg)
0122         vg=ones(kg,p);                                          <span class="comment">% default to diagonal covariances</span>
0123     <span class="keyword">end</span>
0124     <span class="keyword">if</span> nargin&lt;6 || isempty(wg)
0125         wg=repmat(1/kg,kg,1);                                   <span class="comment">% default to uniform weights</span>
0126     <span class="keyword">end</span>
0127     <span class="keyword">if</span> p==1                                                     <span class="comment">% if data dimension is 1</span>
0128         vg=vg(:);                                               <span class="comment">% ... then variances are always diagonal</span>
0129         dvg=true;
0130     <span class="keyword">else</span>
0131         dvg=ismatrix(vg) &amp;&amp; size(vg,1)==kg;                     <span class="comment">% diagonal covariance matrix vg is supplied</span>
0132     <span class="keyword">end</span>
0133     <span class="comment">% first calculate pairwise Bhattacharyya divergences between the components of f and g</span>
0134     dbfg=zeros(kf,kg);                                          <span class="comment">% space for full covariance matrices (overwritten below if f and g both diagonal)</span>
0135     dix=1:p+1:p^2;                                              <span class="comment">% index of diagonal elements in covariance matrix</span>
0136     <span class="keyword">if</span> dvf
0137         <span class="keyword">if</span> dvg                                                  <span class="comment">% both f and g have diagonal covariances</span>
0138             fix=repmat((1:kf)',kg,1);                           <span class="comment">% index into f values</span>
0139             gix=reshape(repmat(1:kg,kf,1),kf*kg,1);             <span class="comment">% index into g values</span>
0140             mdif=mf(fix,:)-mg(gix,:);                           <span class="comment">% difference in means</span>
0141             vfpg=(vf(fix,:)+vg(gix,:));                         <span class="comment">% sum of variances</span>
0142             qldf=0.25*log(prod(vf,2));                          <span class="comment">% 0.25 * log determinants of f components</span>
0143             qldg=0.25*log(prod(vg,2));                          <span class="comment">% 0.25 * log determinants of g components</span>
0144             dbfg=reshape(0.25*sum((mdif./vfpg).*mdif,2)+0.5*log(prod(vfpg,2))-qldf(fix)-qldg(gix),kf,kg);
0145         <span class="keyword">else</span>                                                    <span class="comment">% diagonal f covariance but not g</span>
0146             qldf=0.25*log(prod(vf,2));                          <span class="comment">% precalculate the log determinants for f covariances</span>
0147             <span class="keyword">for</span> jg=1:kg                                         <span class="comment">% loop through g components</span>
0148                 vjg=vg(:,:,jg);                                 <span class="comment">% full covariance matrix for g</span>
0149                 qldg=0.5*log(prod(diag(chol(vjg))));            <span class="comment">% equivalent to 0.25*log(det(vjg))</span>
0150                 <span class="keyword">for</span> jf=1:kf                                     <span class="comment">% loop through f components</span>
0151                     vfg=vjg;                                    <span class="comment">% take full g covariance matrix</span>
0152                     vfg(dix)=vfg(dix)+vf(jf,:);                 <span class="comment">% ... and add diagonal f covariance</span>
0153                     mdif=mf(jf,:)-mg(jg,:);                     <span class="comment">% difference in means</span>
0154                     dbfg(jf,jg)=0.25*(mdif/vfg)*mdif'+log(prod(diag(chol(vfg))))-qldf(jf)-qldg;
0155                 <span class="keyword">end</span>
0156             <span class="keyword">end</span>
0157         <span class="keyword">end</span>
0158     <span class="keyword">else</span>
0159         <span class="keyword">if</span> dvg                                                  <span class="comment">% diagonal g covariance but not f</span>
0160             qldg=0.25*log(prod(vg,2));                          <span class="comment">% precalculate the log determinants for g covariances</span>
0161             <span class="keyword">for</span> jf=1:kf                                         <span class="comment">% loop through f components</span>
0162                 vjf=vf(:,:,jf);                                 <span class="comment">% full covariance matrix for f</span>
0163                 qldf=0.5*log(prod(diag(chol(vjf))));            <span class="comment">% equivalent to 0.25*log(det(vjf))</span>
0164                 <span class="keyword">for</span> jg=1:kg                                     <span class="comment">% loop through g components</span>
0165                     vfg=vjf;                                    <span class="comment">% take full f covariance matrix</span>
0166                     vfg(dix)=vfg(dix)+vg(jg,:);                 <span class="comment">% ... and add diagonal g covariance</span>
0167                     mdif=mf(jf,:)-mg(jg,:);                     <span class="comment">% difference in means</span>
0168                     dbfg(jf,jg)=0.25*(mdif/vfg)*mdif'+log(prod(diag(chol(vfg))))-qldg(jg)-qldf;
0169                 <span class="keyword">end</span>
0170             <span class="keyword">end</span>
0171         <span class="keyword">else</span>                                                    <span class="comment">% both f and g have full covariance matrices</span>
0172             qldg=zeros(kg,1);
0173             <span class="keyword">for</span> jg=1:kg                                         <span class="comment">% precalculate the log determinants for g covariances</span>
0174                 qldg(jg)=0.5*log(prod(diag(chol(vg(:,:,jg))))); <span class="comment">% equivalent to 0.25*log(det(vg(:,:,jg)))</span>
0175             <span class="keyword">end</span>
0176             <span class="keyword">for</span> jf=1:kf                                         <span class="comment">% loop through f components</span>
0177                 vjf=vf(:,:,jf);                                 <span class="comment">% covariance matrix for f</span>
0178                 qldf=0.5*log(prod(diag(chol(vjf))));            <span class="comment">% equivalent to 0.25*log(det(vjf))</span>
0179                 <span class="keyword">for</span> jg=1:kg                                     <span class="comment">% loop through g components</span>
0180                     vfg=vjf+vg(:,:,jg);                         <span class="comment">% calculate sum of covariance matrices</span>
0181                     mdif=mf(jf,:)-mg(jg,:);                     <span class="comment">% difference in means</span>
0182                     dbfg(jf,jg)=0.25*(mdif/vfg)*mdif'+log(prod(diag(chol(vfg))))-qldg(jg)-qldf;
0183                 <span class="keyword">end</span>
0184             <span class="keyword">end</span>
0185         <span class="keyword">end</span>
0186     <span class="keyword">end</span>
0187     dbfg=dbfg-hpl2;                                             <span class="comment">% add correction term to all the calculated covariances</span>
0188     <span class="comment">%</span>
0189     <span class="comment">% Now calculate the variational bound</span>
0190     <span class="comment">% Note that in [3], the psi and phi symbols are interchanged in (20) and also in the previous</span>
0191     <span class="comment">% line; in addition, the subscript of phi is incorrect in the denominator of (26).</span>
0192     <span class="comment">%</span>
0193     lwf=repmat(log(wf),1,kg);                                   <span class="comment">% log of f component weights</span>
0194     lwg=repmat(log(wg'),kf,1);                                  <span class="comment">% log of g component weights</span>
0195     lhf=repmat(log(1/kf),kf,kg);                                <span class="comment">% initialize psi_f|g from [3] (cols of exp(lhf) sum to 1)</span>
0196     lhg=repmat(log(1/kg),kf,kg);                                <span class="comment">% initialize phi_g|f from [3] (rows of exp(lhg) sum to 1)</span>
0197     dbfg2=2*dbfg;                                               <span class="comment">% log of squared Bhattacharyya measure lower bound</span>
0198     dbfg2f=lwf-dbfg2;                                           <span class="comment">% interation-independent term used to update lhg</span>
0199     dbfg2g=lwg-dbfg2;                                           <span class="comment">% interation-independent term used to update lhf</span>
0200     dbfg2fg=dbfg2(:)-lwf(:)-lwg(:);                             <span class="comment">% iteration-independent termto calculate the divergence upper bound</span>
0201     dub=Inf;                                                    <span class="comment">% dummy upper bound for first iteration</span>
0202     <span class="keyword">for</span> ip=1:maxiter                                            <span class="comment">% maximum number of iterations</span>
0203         dubp=dub;                                               <span class="comment">% save previous iteration's upper bound</span>
0204         dub=-<a href="v_logsum.html" class="code" title="function y=v_logsum(x,d,k)">v_logsum</a>(0.5*(lhf(:)+lhg(:)-dbfg2fg));             <span class="comment">% update the upper bound on Bhattacharyya divergence</span>
0205         <span class="keyword">if</span> dub&gt;=dubp                                            <span class="comment">% quit if no longer decreasing</span>
0206             <span class="keyword">break</span>;
0207         <span class="keyword">end</span>
0208         lhg=lhf+dbfg2f;                                         <span class="comment">% update phi_g|f as in numerator of [3]-(25)</span>
0209         lhg=lhg-repmat(<a href="v_logsum.html" class="code" title="function y=v_logsum(x,d,k)">v_logsum</a>(lhg,2),1,kg);                   <span class="comment">% normalize phi_g|f as in [3]-(25) (rows of exp(lhg) sum to 1)</span>
0210         dub=-<a href="v_logsum.html" class="code" title="function y=v_logsum(x,d,k)">v_logsum</a>(0.5*(lhf(:)+lhg(:)-dbfg2fg));             <span class="comment">% update the upper bound on Bhattacharyya divergence</span>
0211         lhf=lhg+dbfg2g;                                         <span class="comment">% update psi_f|g as in numerator of [3]-(26)</span>
0212         lhf=lhf-repmat(<a href="v_logsum.html" class="code" title="function y=v_logsum(x,d,k)">v_logsum</a>(lhf,1),kf,1);                   <span class="comment">% normalize psi_f|g as in [3]-(26) (cols of exp(lhf) sum to 1)</span>
0213     <span class="keyword">end</span>
0214     <span class="keyword">if</span> isempty(nx) || nx==0                                     <span class="comment">% only calculate the upper divergence bound</span>
0215         d=dub;
0216     <span class="keyword">else</span>
0217         [lnwt,jlnwt]=sort(0.5*(lhf(:)+lhg(:)-dbfg2fg)+dub,<span class="string">'descend'</span>); <span class="comment">% normalized component log weights (highest first)</span>
0218         wt=exp(lnwt);
0219         cwt=cumsum(wt);
0220         nmix=1+sum(cwt&lt;1-pruneth/nx);                           <span class="comment">% number of mixtures for &lt;20% chance that any excluded ones would be picked</span>
0221         [fix,gix]=ind2sub([kf kg],jlnwt(1:nmix));               <span class="comment">% mixture indices that are needed</span>
0222         <span class="comment">%</span>
0223         <span class="comment">% now create the sampling GMM distribution</span>
0224         <span class="comment">%</span>
0225         ws=wt(1:nmix)/cwt(nmix);                                <span class="comment">% sampling GMM weight vector</span>
0226         ms=zeros(nmix,p);                                       <span class="comment">% space for sampling GMM means</span>
0227         vs=zeros(p,p,nmix);                                     <span class="comment">% space for sampling GMM full covariances</span>
0228         <span class="keyword">if</span> dvf
0229             <span class="keyword">if</span> dvg                                              <span class="comment">% both f and g have diagonal covariances</span>
0230                 vff=vf(fix,:);
0231                 vgg=vg(gix,:);
0232                 vsumi=1./(vff+vgg);
0233                 vs=2*vff.*vgg.*vsumi;                           <span class="comment">% mixture covariance matrix</span>
0234                 ms=vff.*vsumi.*mg(gix,:)+vgg.*vsumi.*mf(fix,:); <span class="comment">% mixture means</span>
0235             <span class="keyword">else</span>                                                <span class="comment">% diagonal f covariance but not g</span>
0236                 <span class="keyword">for</span> jfg=1:nmix
0237                     vgg=vg(:,:,gix(jfg));
0238                     vff=vf(fix(jfg),:);
0239                     vsum=vgg;
0240                     vsum(dix)=vsum(dix)+vff;                    <span class="comment">% add diagonal components</span>
0241                     vs(:,:,jfg)=2*vgg/vsum.*repmat(vff,p,1);    <span class="comment">% mixture covariance matrix</span>
0242                     ms(jfg,:)=mg(gix(jfg),:)/vsum.*vff+mf(fix(jfg),:)/vsum*vgg; <span class="comment">% mixture means</span>
0243                 <span class="keyword">end</span>
0244             <span class="keyword">end</span>
0245         <span class="keyword">else</span>
0246             <span class="keyword">if</span> dvg                                              <span class="comment">% diagonal g covariance but not f</span>
0247                 <span class="keyword">for</span> jfg=1:nmix
0248                     vff=vf(:,:,fix(jfg));
0249                     vgg=vg(gix(jfg),:);
0250                     vsum=vff;
0251                     vsum(dix)=vsum(dix)+vgg;                    <span class="comment">% add diagonal components</span>
0252                     vs(:,:,jfg)=2*vff/vsum.*repmat(vgg,p,1);    <span class="comment">% mixture covariance matrix</span>
0253                     ms(jfg,:)=mf(fix(jfg),:)/vsum.*vgg+mg(gix(jfg),:)/vsum*vff; <span class="comment">% mixture means</span>
0254                 <span class="keyword">end</span>
0255             <span class="keyword">else</span>                                                <span class="comment">% both f and g have full covariance matrices</span>
0256                 <span class="keyword">for</span> jfg=1:nmix
0257                     vff=vf(:,:,fix(jfg));
0258                     vgg=vg(:,:,gix(jfg));
0259                     vsum=vff+vgg;
0260                     vs(:,:,jfg)=2*vff/vsum*vgg;                 <span class="comment">% mixture covariance matrix</span>
0261                     ms(jfg,:)=mf(fix(jfg),:)/vsum*vgg+mg(gix(jfg),:)/vsum*vff; <span class="comment">% mixture means</span>
0262                 <span class="keyword">end</span>
0263             <span class="keyword">end</span>
0264         <span class="keyword">end</span>
0265         x=<a href="v_randvec.html" class="code" title="function [x,kx]=v_randvec(n,m,c,w,mode)">v_randvec</a>(nx,ms,vs,ws);                               <span class="comment">% draw from sampling distribution</span>
0266         d=-(<a href="v_logsum.html" class="code" title="function y=v_logsum(x,d,k)">v_logsum</a>(0.5*(<a href="v_gaussmixp.html" class="code" title="function [lp,rp,kh,kp]=v_gaussmixp(y,m,v,w,a,b)">v_gaussmixp</a>(x,mf,vf,wf)+<a href="v_gaussmixp.html" class="code" title="function [lp,rp,kh,kp]=v_gaussmixp(y,m,v,w,a,b)">v_gaussmixp</a>(x,mg,vg,wg))-<a href="v_gaussmixp.html" class="code" title="function [lp,rp,kh,kp]=v_gaussmixp(y,m,v,w,a,b)">v_gaussmixp</a>(x,ms,vs,ws)))+log(nx); <span class="comment">% montecarlo estimate of Bhatt divergence</span>
0267     <span class="keyword">end</span>
0268 <span class="keyword">end</span>
0269 <span class="keyword">if</span> ~nargout
0270     <span class="keyword">switch</span> p
0271         <span class="keyword">case</span> 1
0272             nsd=3;          <span class="comment">% number of std deviations to plot</span>
0273             nxax=251; <span class="comment">% number of points on x-axis (MUST be odd)</span>
0274             xlo=min([mf;mg]-nsd*sqrt([vf;vg]));
0275             xhi=max([mf;mg]+nsd*sqrt([vf;vg]));
0276             xax=linspace(xlo,xhi,nxax)';
0277             sint=(xax(2)-xax(1))/3*(4-2*mod(1:nxax,2)-[1 zeros(1,nxax-2) 1]); <span class="comment">% Simpson's rule integration</span>
0278             yf=exp(<a href="v_gaussmixp.html" class="code" title="function [lp,rp,kh,kp]=v_gaussmixp(y,m,v,w,a,b)">v_gaussmixp</a>(xax,mf,vf,wf));
0279             yg=exp(<a href="v_gaussmixp.html" class="code" title="function [lp,rp,kh,kp]=v_gaussmixp(y,m,v,w,a,b)">v_gaussmixp</a>(xax,mg,vg,wg));
0280             bayeserr=sint*min(yf,yg)*0.5; <span class="comment">% calculate Bayes error</span>
0281             plot(xax,yf,<span class="string">'-b'</span>,xax,yg,<span class="string">'-r'</span>,xax,sqrt(yf.*yg),<span class="string">'-g'</span>);
0282             <span class="keyword">if</span> ~isempty(nx) &amp;&amp; nx~=0
0283                 ys=exp(<a href="v_gaussmixp.html" class="code" title="function [lp,rp,kh,kp]=v_gaussmixp(y,m,v,w,a,b)">v_gaussmixp</a>(xax,ms,vs,ws));
0284                 hold on
0285                 plot(xax,exp(-d)*ys,<span class="string">'--k'</span>);
0286                 hold off
0287                 legend(<span class="string">'f(x)'</span>,<span class="string">'g(x)'</span>,<span class="string">'\surd(fg)'</span>,<span class="string">'\approx \surd(fg)'</span>,<span class="string">'location'</span>,<span class="string">'northeast'</span>);
0288                 <a href="v_texthvc.html" class="code" title="function h=v_texthvc(x,y,t,p,q,r)">v_texthvc</a>(0.02,0.98,sprintf(<span class="string">'Bhattacharyya = %.1f%% (&gt;=%.1f%%)\nBayes Err = 0.5 x %.1f%%'</span>,100*exp(-d),100*exp(-dub),200*bayeserr),<span class="string">'LTk'</span>);
0289             <span class="keyword">else</span>
0290                 legend(<span class="string">'f(x)'</span>,<span class="string">'g(x)'</span>,<span class="string">'\surd(fg)'</span>,<span class="string">'location'</span>,<span class="string">'northeast'</span>);
0291             <span class="keyword">end</span>
0292             xlabel(<span class="string">'x'</span>);
0293             ylabel(<span class="string">'Prob density'</span>);
0294     <span class="keyword">end</span>
0295 <span class="keyword">end</span></pre></div>

<hr><address>Generated by <strong><a href="http://www.artefact.tk/software/matlab/m2html/" target="_parent">m2html</a></strong> &copy; 2003</address>
</body>
</html>